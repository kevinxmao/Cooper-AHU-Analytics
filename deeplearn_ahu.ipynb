
#!/usr/bin/env python
# coding: utf-8

# In[2]:


# import libraries for probability distributions and operations, importing files, data manipulation
from scipy.stats import norm
from scipy.stats import multivariate_normal as multinorm
import numpy as np
import math
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import random
import pandas as pd
from pandas import ExcelFile
from sklearn.cluster import DBSCAN as dbs
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

from IPython.display import clear_output
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns

#Tensorflow
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import LeakyReLU

import plotly.graph_objects as go


# In[3]:


filename = 'AHU3_April2019_formatted.csv'
ahu = pd.read_csv(filename, header=0)

ahu_col = ["Date", "Time", "SAT", "CFM", "SP", "OAT_RF", "OAT_BLRS", "OAWB", "CCT", "KW"]
ahu.columns = ahu_col


# In[4]:


# Clustering funtion: DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
# given ahu data and columns, generate a clustering model using DBSCAN
# return the model along with an array for each cluster of data points
# optionally plot a scatter plot of the clusters
def cluster(ahu, cols, plot_clusters = False):
    # get labels from DBSCAN clustering model
    # label indices should be [-1, 0, 1, 2] for [noise (outliers), cluster 1, cluster 2, cluster 3 (outliers)]    
    model = dbs(eps = 0.3, min_samples = 100).fit(ahu[cols])
    labels = model.labels_
    
    # list to store each data cluster (dataframe)
    clusters = []
    
    # apend each cluster independently to the list
    for i in np.unique(labels):
        clusters.append(ahu[labels == i])
    
    # optionally plot clusters
    if plot_clusters:
        plotClusters(clusters, cols)
    
    # return the list of clusters
    return model, clusters

# given list of cluster dataframes and column names
# plot scatter plots of the clusters, in 2D and 3D (including kW target)
def plotClusters(clusters, cols):
    # create plots for 2D and 3D
    ax = plt.axes(projection='3d')
    fig, p = plt.subplots(1)
    
    # scatter plot of each cluster in 2D and 3D
    for c in clusters:
        p.scatter(c[cols[0]], c[cols[1]],s=4)
        ax.scatter(c[cols[0]], c[cols[1]], c['KW'],s=4)
    
    # set legend and axis labels based on column names
    ax.legend(['Noise (Outliers)', 'Cluster 1', 'Cluster 2', 'Cluster 3 (Outliers)'])
    ax.set_xlabel(cols[0])
    ax.set_ylabel(cols[1])
    ax.set_zlabel('kW')
    #plt.savefig('cluster 3D notebook')

    p.legend(['Noise (Outliers)', 'Cluster 1', 'Cluster 2', 'Cluster 3 (Outliers)'])
    p.set_xlabel(cols[0])
    p.set_ylabel(cols[1])
    #plt.savefig('cluster 2D notebook')


# In[5]:


# given training data, trainlinear regression model with evidence approximation
# using the features and bias as the basis functions
# return the mean and variance of the predictive distribution,
# along with matrix containing basis functions and targets, to be used later for sorting 
# optionally track/print current iteration #
def trainLinReg(x_train, t_train, print_it = False):
    # number of parameters (basis functions)
    M = x_train.shape[1]+1

    # number of training data points
    N = len(x_train)

    # S_N and Mn matrix dimensions based on # basis functions + 1
    # initial values for S_N (S_0) and Mn (m_0) matrices based on the initial prior density
    # choose S_0 = some value * I
    # m_0 = array of zeros
    # these values will be updated in each iteration with Eqs. 3.50-3.51
    S_N = np.linalg.inv(np.identity(M))
    Mn = np.zeros((M, 1))

    # # Create MxN Phi matrix from x observations: Eq. 3.16
    # where M = # basis functions and N = # of training data points
    # Phi will store basis functions evaluated at every x observation
    # in this case basis functions are just the original 6 features: "SAT", "CFM", "SP", "OAT_RF", "OAT_BLRS", "OAWB"
    # so Phi is just the tranpose of the x matrix
    Phi = np.zeros((M, N))
    # make first row of Phi matrix contain the bias term, so set to 1, remaining part is the transpose of x_train
    Phi[0] = 1
    Phi[1:] = x_train.T

    # create iota matrix, transpose of the Phi matrix
    iota = Phi.T

    # find the 'best' values of alpha and beta
    # first choose some initial values for alpha and beta
    alpha = 1
    beta = 1

    # tolerange to test for convergence
    tol = 0.01

    # variables to store previous values of alpha and beta, initialize outside of tolerance
    alpha_old = alpha + 10*tol
    beta_old = beta + 10*tol

    # track number of iterations
    num_it = 0

    # iterate until hyperparameters converge within tolerance or max number of iterations is reached to prevent calc errors
    while (abs(alpha-alpha_old) >= tol or abs(beta-beta_old) >= tol) and num_it < 18:
        # increment number of iterations
        num_it += 1

        # optinally print current iteration
        if print_it:
            print("Evidence Approximation: Iteration #%d" % num_it, end="\r")

        # calculate S_N: Eq. 3.53
        S_N = np.linalg.inv(alpha*np.identity(M) + beta*iota.T@iota)

        # compue m_N: Eq. 3.53
        m_N = beta*S_N@iota.T@t_train

        # store values of alpha and beta
        alpha_old = alpha
        beta_old = beta

        # re-estimate alpha: Eq. 3.98, 2 cancels with 1/2 (E_W found using Eq. 3.25)
        alpha = M/np.squeeze(m_N.T@m_N)

        # re-estimate beta: Eq. 3.99 (E_W found using Eq. 3.26)
        # variable to track summation
        E_D = 0

        # iterate through each training data point to compute E_D
        for i in range(0, N):
            # summation from Eq. 3.26, need to convert 1D column array to Mx1 array
            E_D = E_D + (t_train[i] - m_N.T@Phi[:, i].reshape(M, 1))**2

        # compute beta: Eq. 3.99, 2 cancels with 1/2
        beta = N/np.squeeze(E_D)
    
    # return mean and variance of the predictive distribution
    return m_N, S_N, np.append(Phi, t_train.T, axis = 0)

# given mean of predictive distribution and test data
# return predicted target values from linear regression
def predictLinReg(mean, x):
    # initialize Phi matrix with additional row for bias basis function
    Phi = np.zeros((x.shape[1]+1, len(x)))
    
    # make first row of Phi matrix contain the bias term, so set to 1, remaining part is the transpose of data
    Phi[0] = 1
    Phi[1:] = x.T
    
    # compute and return predicted value (mean of predictive distribution): Eq. 3.58
    return np.squeeze(mean.T@Phi)

# given cluster model, means of predictive distribution, and test data
# predict the respective cluster for each data point 
# and return predicted target values from the corresponding linear regression mean
# NOTE: assumes CFM and SP columns are still the only features used in clustering
def predictLinReg_cluster(clusterModel, mean, x_test, cfmcol):
    # empty array to store predicted target values
    t = np.zeros(len(x_test))
    
    # get cluster labels for the test data points
    labels = clusterModel.fit_predict(x_test[:, cfmcol:cfmcol+2])
    
    # iterate through each observation
    for i, x in enumerate(x_test):
        # if current label matches the first cluster label (0), then use the first mean to predict the target
        if labels[i] == 0:
            t[i] = predictLinReg(mean[0], x.reshape((1,len(x))))
        else:
            # otherwise label matches the second cluster (1), so predict using the second mean
            t[i] = predictLinReg(mean[1], x.reshape((1,len(x))))
    # return the target array
    return t


# In[6]:


##################### Linear Regression Model: Original Features + Bias ######################
get_ipython().run_line_magic('matplotlib', 'notebook')
# percentage of data (as decimal) to be used for training the model
per_train = 0.25

# columns to define the range of features used for the linear regression model
col_start = '10^3 CFM'
col_end = 'SP'

# columns to be used for sorting and plotting data
col_sort = 'SP'
col_plot = 'SP'
col_plot3D = '10^3 CFM'

# choose which format to plot regression model results: '2D', '3D' or 'none'
# NOTE: Mean Squared Error plots will not show if set to 3D
plot_lr = '2D'

# flag for performing/plotting cluster predictions in addition to linear regression
predict_cluster = True

# create copy of original AHU dataframe
ahu_data = ahu.copy()

# divide CFM colum by 1000 to get all features within an order of magnitude
ahu_data['CFM'] = ahu['CFM']/1000

# update column names consistent withnew CFM column
ahu_col = ["Date", "Time", "SAT", "10^3 CFM", "SP", "OAT_RF", "OAT_BLRS", "OAWB", "CCT", "KW"]
ahu_data.columns = ahu_col

ahu['Datetime'] = pd.to_datetime(ahu['Date'] + ' ' + ahu['Time'])
ahu = ahu.drop(['Date', 'Time'], axis=1)

# convert column names to dataframe indices (increment sort and plot indices by 1 to account for bias index)
start = (np.where(ahu_data.columns == col_start))[0][0]
end = (np.where(ahu_data.columns == col_end))[0][0]
col_s = (np.where(ahu_data.columns == col_sort))[0][0] - start + 1
col_p = (np.where(ahu_data.columns == col_plot))[0][0] - start + 1
col_p3D = (np.where(ahu_data.columns == col_plot3D))[0][0] - start + 1

# get column containing CFM data for use in the predictLinReg_cluster function
col_cfm = (np.where(ahu_data.columns == '10^3 CFM'))[0][0] - start

# number of basis functions including bias (last column index to be used): min = 3, max value = 7
num_bf = end - start + 2

# get cluster labels for AHU data
# return value should be ordered as follows: cluster model, [noise (outliers), cluster 1, cluster 2, cluster 3 (outliers)]
clusterModel, [noise, cluster1, cluster2, cluster3] = cluster(ahu_data, ['10^3 CFM', 'SP'])

# array to store test data from both clusters
test_data = np.empty((0, num_bf))

# clean the data by omitting the noise and 3rd cluster since they represent outliers
# train a linear regression model for each of the two remaining clusers
# lists to store mean and variance for each of the 2 linear regression models
mean, var = [0]*2, [0]*2

# create linear regression figure plot depending on chosen format
if plot_lr == '2D':
    fig, plot2D = plt.subplots()
elif plot_lr == '3D':
    get_ipython().run_line_magic('matplotlib', 'notebook')
    ax = plt.axes(projection='3d')

# For deep learning
EPOCHS = 1000
#Model Training
class PrintDot(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs):
        if epoch%100==0:
            print('')
        print('.',end='')

# initialize lists to store tf results and labels
SP = []
tf_predictions = []

# Create a dataframe that displays all error
err = {'Cluster':['1', '2'], 'Linear Regression MSE':[0,0], 
       'Deep Learning Train MSE':[0,0], 'Deep Learning Value MSE':[0,0]}
df_err = pd.DataFrame(err)

# iterate through each cluster to perform linear regression
for i, data in enumerate([cluster1, cluster2]):
    # separate data into training and testing portions, excluding date and time columns for now
    # need to convert target arrays to Nx1 column vectors
    x_train, x_test, t_train, t_test = train_test_split(np.array(data.iloc[:,start:end+1]), np.array(data['KW']).reshape(len(data), 1), train_size = per_train)

    # store test data, including targets, to use later
    test_data = np.append(test_data, np.append(x_test, t_test, axis = 1), axis=0)
        
    # list to store mean squared errors for each cluster as number of trianing observations varies
    mse_LR = []

    # iterate through each training data point to get mean squared error values
    for j in range(0, len(x_train)):
        print("Cluster %d: observation %d of %d" % (i+1, j+1, len(x_train)), end = '\r')
        # train linear regression model on current range of training data and store mean and variance in parameters array
        mean[i], var[i], arr_sort = trainLinReg(x_train[:j+1], t_train[:j+1])

        # get sorted versions of training data through arr_sort (basis function + target) matrix for plotting
        # sorted by CFM (index 2)
        arr_sort = arr_sort[:, np.argsort(arr_sort[col_s, :])]
        
        # extract basis functions and targets separately
        phi_sort = arr_sort[0:len(arr_sort) - 1]
        t_sort = arr_sort[len(arr_sort) - 1]

        # get predicted values for test data using current model parameters
        t_pred = predictLinReg(mean[i], x_test)
        
        # get mean squared error between test data targets and predicted targets and append to current cluster MSE list
        mse_LR.append(mean_squared_error(np.squeeze(t_test), t_pred))
    
##################### Deep Learning Model: KERAS ######################
    x = data[['SAT', '10^3 CFM', 'SP', 'OAT_RF', 'OAT_BLRS', 'OAWB', 'CCT']]
    y = data['KW']
    x_train, x_test, t_train, t_test = train_test_split(x, y, test_size=0.20, random_state=101)
    
    x_train_stats = x_train.describe().transpose()
    
    def norm(x):
        return (x - x_train_stats['mean'])/x_train_stats['std']
    normed_x_train = norm(x_train)
    
    ##### Model ####
    
    #Build Model
    def build_model(x):
        model = keras.Sequential([
            layers.Dense(8, input_shape=[len(x.keys())]),
            layers.LeakyReLU(alpha=0.2),
            #layers.Dropout(0.25),
            layers.Dense(7),
            layers.LeakyReLU(alpha=0.2),
            #layers.Dropout(0.25),
            #layers.Dense(256),
            #layers.LeakyReLU(alpha=0.2),
            #layers.Dropout(0.15),
            #layers.Dense(512),
            #layers.LeakyReLU(alpha=0.2),
            #layers.Dropout(0.15),
            #layers.Dense(92, activation=tf.nn.leaky_relu),
            layers.Dense(1),
            #layers.LeakyReLU(alpha=0.2)
        ])

        optimizer = tf.keras.optimizers.Adam(0.008)

        model.compile(loss='mean_squared_error',
                     optimizer = optimizer,
                     metrics=['mean_absolute_error','mean_squared_error'])
        return model

    model = build_model(x_train)
    
    # Save the model
    tf.saved_model.save(model, "/tmp/ahu/cluster%d"%(i+1))

    # Load model
    #model = tf.saved_model.load("/tmp/ahu/cluster%d"%(i+1))
    
    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)
    reduce_LR = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=50)
    history = model.fit(normed_x_train, t_train, epochs=EPOCHS, validation_split=0.2, verbose=0, 
                        callbacks=[early_stop, PrintDot(), reduce_LR], shuffle=True, batch_size=8)
    
    loss, mae, mse_DL = model.evaluate(normed_x_train, t_train, verbose=0)
    
    normed_x_test = norm(x_test)
    
    display_predictions = model.predict(normed_x_test).flatten()
    
    #temp_list = list(display_predictions)
    
    for val in display_predictions:
        tf_predictions.append(val)
    
    for s in x_test['SP']:
        SP.append(s)
    
    # plot mean squared error vs # observations for current cluster
    hist = pd.DataFrame(history.history)
    hist['epoch'] = history.epoch
    fig, plot_mse = plt.subplots(1)
    plot_mse.plot(range(len(mse_LR)), mse_LR, label='Linear Regression Error')
    plot_mse.plot(hist['epoch'], hist['mean_squared_error'], label='Deep Learning Train Error')
    plot_mse.plot(hist['epoch'], hist['val_mean_squared_error'], label='Deep Learning Value Error')
    plot_mse.set_title('Cluster %d' % (i+1))
    plot_mse.set_xlabel('Number of Observations')
    plot_mse.set_ylabel('Mean Squared Error')
    plot_mse.legend()
    
    df_err.iloc[i,1] = mse_LR[-1]
    df_err.iloc[i,2] = hist['mean_squared_error'].iloc[-1]
    df_err.iloc[i,3] = hist['val_mean_squared_error'].iloc[-1]
    
    # optinoally plot either 2D or 3D linear regression model results
    if plot_lr in ['2D', '3D']:
        # compute mean of predictive distribution for plotting
        m_x = np.squeeze(mean[i].T@phi_sort)

        # plot target vs respective feature(s) for 2D or 3D depending on current setting
        # set appropriate title, axis labels
        if plot_lr == '2D':
#             plot2D.set_title('Linear Regression Model Results')
            if i == 0:
                plot2D.plot(phi_sort[col_p,:], m_x, color = 'red')
            else:
                plot2D.plot(phi_sort[col_p,:], m_x, color = 'green')
            plot2D.scatter(phi_sort[col_p,:], t_sort,s=4)
            plot2D.set_xlabel(col_plot)
            plot2D.set_ylabel('kW')
        elif plot_lr == '3D':
#             ax.set_title('Linear Regression Model Results')
            if i == 0:
                ax.plot(phi_sort[col_p,:],phi_sort[col_p3D,:],m_x, color = 'red')
            else:
                ax.plot(phi_sort[col_p,:],phi_sort[col_p3D,:],m_x, color = 'green')
            ax.scatter(phi_sort[col_p,:], phi_sort[col_p3D,:],t_sort,alpha = 0.1,s=4)
            ax.set_xlabel(col_plot)
            ax.set_ylabel(col_plot3D)
            ax.set_zlabel('kW')

display(df_err)


# In[7]:


# set legend for appropriate figure
if plot_lr == '2D':
    plot2D.legend(['Prediction 1', 'Prediction 2', 'Cluster 1', 'Cluster 2'],loc='upper left')
    #plt.savefig('linreg 2 features results notebook')
elif plot_lr == '3D':
    ax.legend(['Prediction 1', 'Prediction 2', 'Cluster 1', 'Cluster 2'])
    #plt.savefig('linreg 2 features results 3D notebook')
    
# if flag is true, then perform predictions of the cluster label as well as target for the test data
if predict_cluster:
    # from the test data matrix, extract only features in test_x, and only targets in test_t
    test_x = test_data[:, 0:test_data.shape[1]-1]
    test_t = test_data[:, test_data.shape[1]-1]

    # get predicted target values using cluster/lin reg prediction model
    test_pred = predictLinReg_cluster(clusterModel, mean, test_x, col_cfm)
    
    # create another figure to plot target vs feature, set title, axis labels, legend
    # SP is the column after CFM
    fig, p = plt.subplots()
    p.scatter(test_x[:, col_cfm+1], test_t,s=4, label='Actual')
    p.scatter(test_x[:, col_cfm+1], test_pred,s=4, label='LR Prediction')
    p.scatter(SP, tf_predictions,s=4, label='DL Prediction')
#     p.set_title('Cluster and Linear Regression Prediction Results')
    p.set_xlabel("Static Pressure [psi]")
    p.set_ylabel('Power [kW]')
    #p.set_title("AHU-3 Power Consumption vs. Static Pressure")
    p.set_ylim(0,25)
    p.legend(loc="upper left")
    #plt.savefig('prediction 2 features notebook')
